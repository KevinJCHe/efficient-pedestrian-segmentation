{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-2HY8R1NjhJ",
    "outputId": "524eaaa2-7974-4de4-e5b1-dd7c2ca0c34d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba1TNgvw0U6J"
   },
   "source": [
    "# CityScape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pc5pHSpOb8bI",
    "outputId": "ceacb488-edb3-4b27-f909-9dcb343aaae2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB7MkqzecAi6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# extract CityScape dataset (after manually uploading it to session)\n",
    "!unzip /content/drive/My\\ Drive/AER1515\\ Project/final_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2ZuVa3RcERV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHF33TTbcioW"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dJ2yQWS2k92"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"final_data/leftImg8bit_trainvaltest/leftImg8bit\"\n",
    "LABEL_PATH = \"final_data/gtFine_trainvaltest/gtFine\"\n",
    "MODEL_CHECKPOINT = \"ModelCheckpoint\"\n",
    "PED_ID = 24\n",
    "RIDER_ID = 25\n",
    "BATCH = 32\n",
    "IMAGE_SIZE = 256\n",
    "EPOCHS = 200\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woJmdxwG0Vw5"
   },
   "outputs": [],
   "source": [
    "def load_data(img_path, label_path):\n",
    "    x = sorted(glob(os.path.join(img_path, \"*/*/*_leftImg8bit.png\")))\n",
    "    y = sorted(glob(os.path.join(label_path, \"*/*/*_labelIds.png\")))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhL7Z8vE1wEQ"
   },
   "outputs": [],
   "source": [
    "imgs, labels = load_data(IMAGE_PATH, LABEL_PATH)\n",
    "assert(all(imgs[idx].split(\"_leftImg8bit\")[0].split(\"leftImg8bit/\")[-1] == \n",
    "           labels[idx].replace(\"_gtFine_\", \"_\").split(\"_labelIds\")[0].split(\"gtFine/\")[-1] \n",
    "           for idx in range(len(imgs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "8A3-5JTe3ZOk",
    "outputId": "c44f4bd8-752a-4819-c89c-f442955d74bf"
   },
   "outputs": [],
   "source": [
    "x = imgs[1]\n",
    "x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1Puhm5E3--u"
   },
   "outputs": [],
   "source": [
    "y = labels[1]\n",
    "y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "y = (y == PED_ID) | (y == RIDER_ID)\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "Qg2u-6fHTK69",
    "outputId": "820a38de-3ab6-4033-8e08-ca364330733c"
   },
   "outputs": [],
   "source": [
    "h, w = y.shape\n",
    "for row in range(h):\n",
    "    for col in range(w):\n",
    "        if not y[row][col]:\n",
    "            x[row][col] = [255, 255, 255]\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm52-62gRRhZ"
   },
   "outputs": [],
   "source": [
    "def load_data(images, masks, split=0.1):\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "    test_size = int(split * total_size)\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "def downsize(x):\n",
    "    # downsample to largest side at 256\n",
    "    x = T.functional.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return x\n",
    "    \n",
    "def read_image(path, train=False):\n",
    "    if train:\n",
    "        path = path.decode()\n",
    "    x = read_and_rgb(path)\n",
    "    x = np.array(downsize(Image.fromarray(x)))\n",
    "    x = x/255.0\n",
    "\n",
    "    return x\n",
    "\n",
    "def read_mask(path, train=False):\n",
    "    if train:\n",
    "        path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = np.array(downsize(Image.fromarray(x)))\n",
    "    x = (x == PED_ID) | (x == RIDER_ID)\n",
    "    x = x.astype(float)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "\n",
    "    return x\n",
    "\n",
    "def read_and_rgb(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x, train=True)\n",
    "        y = read_mask(y, train=True)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
    "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWj3CFUtSjmG"
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(imgs, labels, split=0.1)\n",
    "\n",
    "train_steps = len(train_x)//BATCH\n",
    "valid_steps = len(valid_x)//BATCH\n",
    "\n",
    "if len(train_x) % BATCH != 0:\n",
    "    train_steps += 1\n",
    "if len(valid_x) % BATCH != 0:\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgjBQ5U4X-Ax"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf_dataset(train_x, train_y, batch=BATCH)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=BATCH)\n",
    "test_dataset = tf_dataset(test_x, test_y, batch=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvlt8Q7AugAt",
    "outputId": "92841580-9d37-462b-9452-3da6b0e46351"
   },
   "outputs": [],
   "source": [
    "len(train_x), len(valid_x), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mscY-2GBctcD"
   },
   "outputs": [],
   "source": [
    "def PedSegmentModel():\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "\n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False)\n",
    "    \n",
    "    # model v1\n",
    "    # skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    # encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    # f = [16, 32, 48, 64]\n",
    "\n",
    "    # model v2\n",
    "    # skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\", \"block_13_expand_relu\"]\n",
    "    # encoder_output = encoder.get_layer(\"block_16_expand_relu\").output\n",
    "    # f = [16, 32, 48, 64, 80]\n",
    "\n",
    "    # model v3\n",
    "    # skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    # encoder_output = encoder.get_layer(\"block_10_expand_relu\").output\n",
    "    # f = [16, 32, 48, 64]\n",
    "\n",
    "    # model v4\n",
    "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_7_expand_relu\").output\n",
    "    f = [16, 32, 48, 64]\n",
    "    \n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "smooth = 1e-15\n",
    "def IOU(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + smooth)\n",
    "\n",
    "def IOU_loss(y_true, y_pred):\n",
    "    return 1.0 - IOU(y_true, y_pred)\n",
    "    # return tf.keras.losses.MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSsBD8Drc7Ob"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "metrics = [IOU, Recall(), Precision()]\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False),\n",
    "    ModelCheckpoint(filepath=MODEL_CHECKPOINT, save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True),\n",
    "    TensorBoard(log_dir=\"logs/\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBKmfku3lFQ2",
    "outputId": "5f637db3-dba8-4fd1-ca8c-f139ef63e99c"
   },
   "outputs": [],
   "source": [
    "model = PedSegmentModel()\n",
    "model.compile(loss=IOU_loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy0fDRvEazbk",
    "outputId": "a450e700-3579-477c-f41d-53e8404dd09a"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(MODEL_CHECKPOINT)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebAYqee46dSi",
    "outputId": "b91a5426-debf-4b64-fbcf-08aa2eb7a861"
   },
   "outputs": [],
   "source": [
    "test_steps = (len(test_x)//BATCH)\n",
    "if len(test_x) % BATCH != 0:\n",
    "    test_steps += 1\n",
    "\n",
    "model.load_weights(MODEL_CHECKPOINT)\n",
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KJbx10YQdDLn",
    "outputId": "45fcda09-e41a-4595-aabb-df4c6049b878"
   },
   "outputs": [],
   "source": [
    "def overlay_img_on_mask(img, mask):\n",
    "    for row in range(h):\n",
    "        for col in range(w):\n",
    "            if not mask[row][col][0]:\n",
    "                img[row][col] = [0, 0, 0]\n",
    "                \n",
    "def mask_parse(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask\n",
    "\n",
    "show_x = [test_x[11], test_x[44], test_x[102], test_x[106], test_x[500]]\n",
    "show_y = [test_y[11], test_y[44], test_y[102], test_y[106], test_y[500]]\n",
    "# show_x = test_x[:10]\n",
    "# show_y = test_y[:10]\n",
    "for i, (x, y) in enumerate(zip(show_x, show_y)):\n",
    "    x = read_image(x)\n",
    "    y = read_mask(y)\n",
    "    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n",
    "    h, w, _ = x.shape\n",
    "    white_line = np.ones((h, 10, 3))\n",
    "\n",
    "    overlay_img = copy.deepcopy(x)\n",
    "    overlay_img_on_mask(overlay_img, y_pred)\n",
    "\n",
    "    all_images = [\n",
    "        x, white_line,\n",
    "        mask_parse(y), white_line,\n",
    "        mask_parse(y_pred), white_line,\n",
    "        overlay_img\n",
    "    ]\n",
    "\n",
    "    image = np.concatenate(all_images, axis=1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    a = fig.add_subplot(1, 1, 1)\n",
    "    imgplot = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56uKGadF6B8j",
    "outputId": "75012192-8a12-4d53-c66b-81542ab58e21"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "cp /content/ModelCheckpoint.data-00000-of-00001 /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint.data-00000-of-00001\n",
    "cp /content/ModelCheckpoint.index /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jf8ryA8ocsE"
   },
   "source": [
    "# COMPARE MODELs FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfagH0S32ZyG",
    "outputId": "f9946ea9-5361-4874-ac9d-a124547165c5"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_13.data-00000-of-00001 ModelCheckpoint_13.data-00000-of-00001\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_13.index ModelCheckpoint_13.index\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_16.data-00000-of-00001 ModelCheckpoint_16.data-00000-of-00001\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_16.index ModelCheckpoint_16.index\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_10.data-00000-of-00001 ModelCheckpoint_10.data-00000-of-00001\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_10.index ModelCheckpoint_10.index\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_7.data-00000-of-00001 ModelCheckpoint_7.data-00000-of-00001\n",
    "cp /content/drive/My\\ Drive/AER1515\\ Project/ModelCheckpoint_7.index ModelCheckpoint_7.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2BDWsfxo9iw"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YGJxh3qqzxx"
   },
   "outputs": [],
   "source": [
    "def downsize(x):\n",
    "    # downsample to largest side at 256\n",
    "    x = T.functional.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return x\n",
    "    \n",
    "def read_image(path):\n",
    "    x = read_and_rgb(path)\n",
    "    x = np.array(downsize(Image.fromarray(x)))\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = np.array(downsize(Image.fromarray(x)))\n",
    "    x = (x == PED_ID) | (x == RIDER_ID)\n",
    "    return x\n",
    "\n",
    "def read_and_rgb(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    return x\n",
    "\n",
    "def overlay_img_on_mask(img, mask):\n",
    "    for row in range(h):\n",
    "        for col in range(w):\n",
    "            if not mask[row][col]:\n",
    "                img[row][col] = [0, 0, 0]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf8Uidpe000h"
   },
   "outputs": [],
   "source": [
    "def PedSegmentModel(version):\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "\n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False) #, alpha=0.35)\n",
    "    \n",
    "    if version == 0:\n",
    "        skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "        encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "        f = [16, 32, 48, 64]\n",
    "\n",
    "    elif version == 1:\n",
    "        skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\", \"block_13_expand_relu\"]\n",
    "        encoder_output = encoder.get_layer(\"block_16_expand_relu\").output\n",
    "        f = [16, 32, 48, 64, 80]\n",
    "\n",
    "    elif version == 2:\n",
    "        skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "        encoder_output = encoder.get_layer(\"block_10_expand_relu\").output\n",
    "        f = [16, 32, 48, 64]\n",
    "    \n",
    "    elif version == 3:\n",
    "        skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "        encoder_output = encoder.get_layer(\"block_7_expand_relu\").output\n",
    "        f = [16, 32, 48, 64]\n",
    "    \n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "BttWi1EUyu0Q",
    "outputId": "d0aa4c53-ca03-49f3-daa0-dfc568d2e0ea"
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "PEDID = 15\n",
    "fcn_101 = torchvision.models.segmentation.fcn_resnet101(pretrained=True)\n",
    "fcn_50 = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n",
    "fcn_101.cuda()\n",
    "fcn_50.cuda()\n",
    "def fcn_predict(fcn, x):\n",
    "    inp = transform(x)\n",
    "    inp = inp.unsqueeze(0)\n",
    "    inp = inp.cuda()\n",
    "    y_pred = fcn(inp)['out']\n",
    "    y_pred = y_pred.squeeze(0)\n",
    "    y_pred = torch.argmax(y_pred, dim=0).detach().cpu().numpy()\n",
    "    y_pred = y_pred == PEDID\n",
    "    return y_pred\n",
    "\n",
    "checkpoint_map = {0: \"ModelCheckpoint_13\", 1: \"ModelCheckpoint_16\", 2: \"ModelCheckpoint_10\", 3: \"ModelCheckpoint_7\"}\n",
    "def model_predict(x, version):\n",
    "    model = PedSegmentModel(version)\n",
    "    model.load_weights(checkpoint_map[version])\n",
    "    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOrOlGQLpgCL",
    "outputId": "01d0caf9-e939-4b5d-845d-e1e3671f50a0"
   },
   "outputs": [],
   "source": [
    "show_x = [test_x[11], test_x[44], test_x[102], test_x[106], test_x[500], test_x[475], test_x[451], test_x[485], test_x[389]]\n",
    "show_y = [test_y[11], test_y[44], test_y[102], test_y[106], test_y[500], test_y[475], test_y[451], test_y[485], test_y[389]]\n",
    "images = None\n",
    "for i, (x, y) in enumerate(zip(show_x, show_y)):\n",
    "    x = read_image(x)\n",
    "    y = read_mask(y)\n",
    "\n",
    "    y_pred_fcn_50 = fcn_predict(fcn_50, x)\n",
    "    y_pred_fcn_101 = fcn_predict(fcn_101, x)\n",
    "    y_pred_mv0 = model_predict(x/255, version=0)\n",
    "    y_pred_mv1 = model_predict(x/255, version=1)\n",
    "    y_pred_mv2 = model_predict(x/255, version=2)\n",
    "    y_pred_mv3 = model_predict(x/255, version=3)\n",
    "\n",
    "    h, w, _ = x.shape\n",
    "    white_line = np.ones((10, 256, 3))\n",
    "    long_white_line = np.ones(((256+10)*8, 10, 3))\n",
    "\n",
    "    all_images = [\n",
    "        x/255, white_line,\n",
    "        mask_parse(y), white_line,\n",
    "        mask_parse(y_pred_mv3), white_line,\n",
    "        mask_parse(y_pred_mv2), white_line,\n",
    "        mask_parse(y_pred_mv0), white_line,\n",
    "        mask_parse(y_pred_mv1), white_line,\n",
    "        mask_parse(y_pred_fcn_50), white_line,\n",
    "        mask_parse(y_pred_fcn_101), white_line,\n",
    "    ]\n",
    "\n",
    "    image = np.concatenate(all_images, axis=0)\n",
    "\n",
    "    image = [long_white_line, image] if images is None else [images, long_white_line, image]\n",
    "    images = np.concatenate(image, axis=1)\n",
    "    \n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "imgplot = plt.imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fTw3rjepoJi"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx0GH5YjpWEJ",
    "outputId": "7bfed122-2613-4019-e967-c34460de8461"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "m = model\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "dummy_input = np.random.rand(1,256,256,3)\n",
    "\n",
    "# WARM-UP\n",
    "for _ in range(10):\n",
    "   _ = m(dummy_input)\n",
    "\n",
    "# MEASURE PERFORMANCE\n",
    "t = 0\n",
    "repetitions = 300\n",
    "for r in range(repetitions):\n",
    "    start_time = time.time()\n",
    "    result = model(dummy_input)\n",
    "    end_time = time.time()\n",
    "    t += (end_time - start_time)*1000\n",
    "print(\"Mean inference time:\", t/repetitions, \" ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7t0zmUOXVsf"
   },
   "source": [
    "# Tensorboard\n",
    "\n",
    "If not working, just download the logs and open it in a local jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mc9MuCiPphn"
   },
   "source": [
    "# Final Comaprisons\n",
    "\n",
    "My model v1 (13, input,1,3,6): \n",
    "- number of param = 1,265,729\n",
    "- model size = 14.8 MB\n",
    "- mean inference time = 37.2 ms\n",
    "- test IOU = 78.7%\n",
    "\n",
    "My model v2 (16, input,1,3,6,13)\n",
    "- number of param = 3,050,401\n",
    "- model size = 35.6 MB\n",
    "- mean inference time = 44.6 ms\n",
    "- test IOU = 79.5%\n",
    "\n",
    "My model v3 (10, input,1,3,6)\n",
    "- number of param = 813,761\n",
    "- model size = 9.5 MB\n",
    "- mean inference time = 31.16 ms\n",
    "- test IOU = 76.6%\n",
    "\n",
    "My model v4 (7, input,1,3,6)\n",
    "- number of param = 645,953\n",
    "- model size = 7.6 MB\n",
    "- mean inference time = 23.81 ms\n",
    "- test IOU = 75.7%\n",
    "\n",
    "ResNet101 - FCN (pretrained)\n",
    "- number of param = 54,314,346\n",
    "- model size 208 MB\n",
    "- mean inference time = 34.42ms\n",
    "- test IOU = 52.4%\n",
    "\n",
    "ResNet50 - FCN (pretrained)\n",
    "- number of param = 35,322,218\n",
    "- model size 136 MB\n",
    "- mean inference time = 20.07ms\n",
    "- test IOU = 52.6%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "W7FoPkH-jn8j",
    "8SXprBKVdwHJ",
    "f-pOJwrhnjQf",
    "dsIKcwbeCjPK",
    "_fTw3rjepoJi",
    "c7t0zmUOXVsf",
    "2jf8ryA8ocsE",
    "zxu_1LQY1ucE",
    "RCkl4iGBlJU1",
    "UHd4rWm_oIRL",
    "4dyKdN5yjpj5",
    "Q4ZYIAzCzsmi"
   ],
   "name": "AER1515 Trial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
